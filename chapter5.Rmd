# Dimensionality reduction techniques (Week 5)

In this Analysis exercise, I analyze two data sets with dimensionality reduction techniques. The first data set describes peoples' well being in different countries and the second one tea consumption.

## Loading the data

First, I load the human data set and explore its structure and dimensions,

```{r}

# load libraries
library(dplyr)
library(readr)

# load the data
human <- read_csv("data/human.csv")

# print out structure and dimenstions
str(human)

```

The data combines several indicators from most countries in the world. The variables are

- `Country`: Country name,
- `Edu2.FM`: Ratio of proportions of females and males with at least secondary education,
- `Labo.FM`: Ratio of proportions of females and males in the labour force,
- `Life.Exp`: Life expectancy at birth,
- `Edu.Exp`: Expected years of schooling,
- `GNI`: Gross National Income per capita,
- `Mat.Mor`: Maternal mortality ratio,
- `Ado.Birth`: Adolescent birth rate,
- `Parli.F`: Percetange of female representatives in parliament.


## Inspecting the data

Let's begin by inspecting the data set. First I am going to mode the country names to row names,

```{r}

# move the country names to row names
library(tibble)
human <- column_to_rownames(human, "Country")

```

and then print out the summaries of the variables,

```{r}
# print out summaries
summary(human)

```

and finally draw some plots that visualize the distributions of the variables and correlations between them,

```{r}

# Access GGally
library(GGally)

# visualize the 'human_' variables
ggpairs(human, progress = FALSE)

# Access corrplot
library(corrplot)

# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot(method="circle", type="upper", cl.pos="b", tl.pos = "d", tl.cex=0.6)

```

Most of the variables seem to be (almost) normally distributed. The strongest correlation is between `Edu.Exp` and `Life.Exp`, which makes sense: in countries where people live longer, they have more time to educate themselves. The second strongest correlation is between `Mat.Mor` and `Ado.Birth` suggesting that in countries where people give birth at young age, mothers have a higher risk to die when giving birth. This is probably related to the quality and availability of healthcare and birth control. The strongest anti-correlation is between `Life.Exp` and `Mat.Mor`, again most likely reflecting the quality of healthcare in a given country. I can also observe that `Labo.FM` and `Parli.F` have a quite weak connection to the other variables.

## PCA with raw data

Next, I perform a principal component analysis on the raw data set using `prcomp`, print out the summary that shows the variability captured by the principal components, and then draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables,

```{r fig.cap = "**Figure 1**: Biplot of PCA of the raw data set. The first principal component explains all the variance in the data, corresponding to the gross national income per capita of a given country."}

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)

# print out the summary of the pca
s <- summary(pca_human)
s

# rounded percentanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.6, 0.8), col = c("grey60", "darkblue"), xlab = pc_lab[1], ylab = pc_lab[2])

```

With the raw data, all the variance accumulates to one principal component. This might be related to the fact that the numerical values of `GNI` are much larger compared to those of the other variables. Standardizing the data set should fix this.

## PCA with standardized data

Let's standardize the data set and perform the PCA again,

```{r fig.cap = "**Figure 2**: Biplot of PCA of the standardized data set. Maternal mortality rate and adolescent birth rate correlate with the first principal component and the quality of education, life expectancy, and gross national income per capita anti-correlate with it. The variables describing gender inequality are orthogonal to these and contribute to the second principal component meaning that gender inequality is an independent feature with respect to healthcare, education, and gross national income. The first PC explains over half of the variance in the data and the second one about 16%."}

# standardize the variables
human_std <- scale(human)

# print out the summaries of the standardized variables
summary(human_std)

# perform principal component analysis (with the SVD method)
pca_human_std <- prcomp(human_std)

# print out the summary of the pca
s_std <- summary(pca_human_std)
s_std

# rounded percentanges of variance captured by each PC
pca_pr_std <- round(100*s_std$importance[2, ], digits = 1)

# create object pc_lab to be used as axis labels
pc_lab_std <- paste0(names(pca_pr_std), " (", pca_pr_std, "%)")

# draw a biplot
biplot(pca_human_std, cex = c(0.6, 0.8), col = c("grey60", "darkblue"), xlab = pc_lab_std[1], ylab = pc_lab_std[2])

```

Now that the data set is standardized and the numerical values of the variables are all similar, the PCA works much better compared to the one with raw data. Without standardizing, all the variance is explained by the first principal component to which the only contributor is the gross national income per capita, `GNI`, due to its large numerical values. The standardization shifts and rescales the variables so that on the second try I can see the correlations much clearer.



## Interpreting the principal components

In Fig. 2, I plot the result of the principal component analysis of the human data set in terms of the two most significant principal components. In this case, the two principal component dimensions have a quite clear interpretation. The first one, responsible for over half of the variation in the data, is strongly related to the health-related features (`Life.Exp`, `Mat.Mor`, `Ado.Birth`) and the quality of education (`Edu.Exp`, `Edu2.FM`) as well as gross national income per capita (`GNI`), since the corresponding arrows are parallel to the first axis. It clearly describes the poorness of a country: if a given country has a large PC1, its GNI is low and its healthcare and education have a low quality. Here, the low quality of healthcare and poorness is reflected by a low life expectancy, a high maternal mortality rate, and a high adolescent birth rate. The low quality of education, on the other hand, is reflected by low expected years of schooling and a low female-to-male ratio in secondary education. It makes sense that these variables are related: poorer countries with lower GNI have less money to spend on healthcare, birth control, and education leading to lower quality of life. And vice versa, lower quality of life probably leads to low GNI.

The second principal component, responsible of 16% of the variation in the data, describes gender inequality in labour force and parliament (`Labo.FM`, `Parli.F`), as the corresponding arrows align with the second axis. Higher (lower) PC2 can be interpreted as higher inequality in favor of females (males) in labour force and parliament. Also, not surprisingly, the female-to-male ratio in labour force and parliament are strongly correlated with each other.

Already suggested by the correlation plot in the beginning, it seems that gender inequality is not connected to the poorness of a country since the arrows describing these two qualities in Fig. 2 are almost perpendicular to each other. I can't come up with a simple explanation why that's the case but it is still an interesting observation.

## MCA on tea data

As the second data set, I analyze tea consumption containing categorical variables. The tool for this is the Multiple Correspondence Analysis (MCA), a multivariate statistical technique used for analyzing relationships in categorical data. To begin, I load the data set and print out its structure and dimensions,

```{r}

# loading the tea data
tea <- read.csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/tea.csv", stringsAsFactors = TRUE)

# print out the structure and dimensions
str(tea)

# view contents of the data
View(tea)

```

The data contains 300 observations of 36 variables collected with a questionnaire on tea consumption. In particular, the participants were asked how they drink tea (18 questions) the what are their product's perception (12 questions). In addition, some personal details were asked (4 questions). The number of variables is fairly high, and I want to focus only on some of them. I choose the following variables:

- `Tea`: Type of tea,
- `How`: Something mixed with tea,
- `how`: Bagged tea or unpackaged tea or both,
- `sugar`: Sugar with tea,
- `where`: Place of purchase,
- `lunch`: Drank at lunch.

So, let's keep only the chosen variables in the data set and visualize their distributions,

```{r}

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea <- dplyr::select(tea, keep_columns)

# visualize the dataset
library(tidyr)
library(ggplot2)
pivot_longer(tea, cols = everything()) %>% 
  ggplot(aes(value)) + facet_wrap("name", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

```

Next, I perform the MCA for the tea data using the `MCA` function from the `FactoMineR` library and print out the summary of the analysis,

```{r}

# multiple correspondence analysis
library(FactoMineR)
mca <- MCA(tea, graph = FALSE)

# summary of the model
summary(mca)

```

Finally, let's plot the result of the MCA as a biplot including only the different categories and hiding the individual observations (with `invisible=c("ind")`),

```{r}

# visualize MCA
plot(mca, invisible=c("ind"), graph.type = "classic", habillage = "quali")

```

The axes in MCA biplot represent the principal components or dimensions and each axis captures a certain proportion of the total variability in the categorical data. In this case, I plot the two dimensions that have the largest contribution to the variance in the data (15% and 14%). The first dimension appears to describe the place of purchase and the type of the tea. Larger values mean that the tea has been purchased from a tea shop and the type is unpackaged and lower values correspond to chain stores as the buying place and tea bags as the type. This also indicates that tea shops sell more unpackaged tea and chain stores sell more tea bags.

The interpretation of the second dimension is not so obvious. One possible interpretation is that it describes how picky or “serious" a tea consumer is: larger values correspond to less serious drinker that buys their tea from any shop (either chain store or tea shop) and might even drink it by mixing it with some other ingredient. On the other hand, lower values correspond to a serious tea consumer that buys their tea (preferably green) only from a proper tea shop.





















