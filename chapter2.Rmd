# Analysis of learning data (Week 2)

*Describe the work you have done this week and summarize your learning.*

-   Describe your work and results clearly.
-   Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
-   Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

In this Analysis exercise, I..

## Loading data

First, I load the data with the `read_csv` function from the `readr` library and print its structure and dimensions:

```{r}
# load the readr library
library(readr)

# load the data file
learning2014 <- read_csv("data/learning2014.csv")

# print the structure
str(learning2014)

# print the dimensions
dim(learning2014)
```

The data consists of 7 variables and 166 observations. The variables are: `gender`, `age`, `attitude`, `deep`, `stra`, `surf`, and `points`. The first two variables are self-explanatory and rest are defined as

- `attitude`: Attitude towards statistics on a scale from 1 to 5. Obtained as a mean of answers to ten questions.
- `deep`: Mean of answers to twelve questions related to deep level learning, in particular seeking meaning, relating ideas, and using evidence. Scale: 1-5.
- `surf`: Mean of answers to twelve questions related to surface level learning, specifically the lack of purpose, unrelated memorizing, and syllabus-boundedness. Scale: 1-5.
- `stra`: Mean of answers to eight questions about how strategic the approach of the student is, assessing organization and time-management. Scale: 1-5.
- `points`: Points obtained from the course exam.

## Visualizing data

Next, I show a graphical overview and summary of the data

```{r, message = FALSE}
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# create a plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)), upper = list(continuous = wrap("cor", size = 2.5)))

# draw the plot
p

# print a summary
summary(learning2014)
```

The dataset mostly includes young students with median age at 22. Also female participants have a clear majority over male ones. Between different variables, the strongest correlation is between `attitude` and `points` irrespective of gender, which indicates that a motivated student is likely to perform well in the course exam. On the other hand, the strongest anti-correlation is between `surf` and `deep`, suggesting that the two attributes are exclusive on some level. This anti-correlation is much higher among males than females, which I can't explain.

## Fitting a linear regression model

Next, I fit a linear regression model, where `points` is the target variable and `attitude`, `stra`, and `surf` are the explanatory variables. These three variables are chosen as explanatory since they have the highest absolute correlation with the target variable. Fitting the model and printing out the summary gives

```{r}
# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + surf, data = learning2014)

# print out a summary of the model
summary(my_model)

```

According to the regression model, only one of the three variables, namely `attitude`, has a statistically significant relationship with the target variable `points`. Its P-value, a particular statistical test measuring the accuracy of the fitted model parameters, is remarkably low, meaning a high significance. The P-values of the other two variables are high, indicating that there is barely any statistical significance in their relationship to `points`.

By removing the insignificant variables from the regression model, I perform the fitting with only one explanatory variable `attitude`:

```{r}
# create a regression model with one explanatory variable
my_model2 <- lm(points ~ attitude, data = learning2014)

# print out a summary of the model
summary(my_model2)

```

This time, the fit is even better with lower P-value. Seems like `attitude` affects `points` with a high statistical significance. On average, for each unit `attitude`, `points` increases by 3.5255 with a standard error of 0.5674. For vanishing `attitude` (by extrapolating it to 0), the expected `points` is 11.6372 with a standard error of 1.8303.

To visualize the linear regression model, I draw a plot:

```{r, message=FALSE}
# Access the gglot2 library
library(ggplot2)

# initialize plot with data and aesthetic mapping
p2 <- ggplot(learning2014, aes(x = attitude, y = points))

# define the visualization type (points)
p2 <- p2 + geom_point()

# plot the linear regression model
p2 <- p2 + geom_smooth(method='lm', color='turquoise4')

# change theme
p2 <- p2 + theme_minimal()

# add title
p2 <- p2 + labs(title="Linear model of points vs attitude")

# adjust title
p2 <- p2 + theme(plot.title = element_text(hjust=0.5, size=16, face='bold')) 

# draw the plot
p2

```


## Interpretation of the fitted model

The multiple R-squared, which in this case with a single explanatory variable is simply the square of the correlation coefficient between `attitude` and `points`, is 0.1906, so that less than 20% of the variation in `points` is explained by the variation in `attitude`.

## Diagnostic plots



```{r}
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))

```
















