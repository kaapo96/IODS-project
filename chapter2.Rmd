# Analysis of learning data (Week 2)

In this Analysis exercise, I analyze the data obtained from a survey studying the influence of different learning methods and personal attitudes to the performance in a course exam on a statistics course lectured in 2014.

## Loading the data

First, I load the data with the `read_csv` function from the `readr` library and print its structure and dimensions:

```{r}
# load the readr library
library(readr)

# load the data file
learning2014 <- read_csv("data/learning2014.csv")

# print the structure
str(learning2014)

# print the dimensions
dim(learning2014)
```

The data consists of 7 variables and 166 observations. The variables are: `gender`, `age`, `attitude`, `deep`, `stra`, `surf`, and `points`. The first two variables are self-explanatory and rest are defined as

-   `attitude`: Attitude towards statistics on a scale from 1 to 5. Obtained as a mean of answers to ten questions.
-   `deep`: Mean of answers to twelve questions related to deep level learning, in particular seeking meaning, relating ideas, and using evidence. Scale: 1-5.
-   `surf`: Mean of answers to twelve questions related to surface level learning, specifically the lack of purpose, unrelated memorizing, and syllabus-boundedness. Scale: 1-5.
-   `stra`: Mean of answers to eight questions about how strategic the approach of the student is, assessing organization and time-management. Scale: 1-5.
-   `points`: Points obtained from the course exam.

## Visualizing data

Next, I show a graphical overview and summary of the data

```{r, message = FALSE}
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# create a plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)), upper = list(continuous = wrap("cor", size = 2.5)))

# draw the plot
p

# print a summary
summary(learning2014)
```

The dataset mostly includes young students with median age at 22. Also female participants have a clear majority over male ones. Between different variables, the strongest correlation is between `attitude` and `points` irrespective of gender, which indicates that a motivated student is likely to perform well in the course exam. On the other hand, the strongest anti-correlation is between `surf` and `deep`, suggesting that the two attributes are exclusive on some level. This anti-correlation is much higher among males than females, which I can't explain.

## Fitting a linear regression model

Next, I fit a linear regression model, where `points` is the target variable and `attitude`, `stra`, and `surf` are the explanatory variables. These three variables are chosen as explanatory since they have the highest absolute correlation with the target variable. Fitting the model and printing out the summary gives

```{r}
# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + surf, data = learning2014)

# print out a summary of the model
summary(my_model)

```

According to the regression model, only one of the three variables, namely `attitude`, has a statistically significant relationship with the target variable `points`. Its P-value, a particular statistical test measuring the accuracy of the fitted model parameters, is remarkably low, meaning a high significance. The P-values of the other two variables are high, indicating that there is barely any statistical significance in their relationship to `points`.

By removing the insignificant variables from the regression model, I perform the fitting with only one explanatory variable `attitude`:

```{r}
# create a regression model with one explanatory variable
my_model2 <- lm(points ~ attitude, data = learning2014)

# print out a summary of the model
summary(my_model2)

```

To visualize the linear regression model, I draw a plot:

```{r, message=FALSE}
# Access the gglot2 library
library(ggplot2)

# initialize plot with data and aesthetic mapping
p2 <- ggplot(learning2014, aes(x = attitude, y = points))

# define the visualization type (points)
p2 <- p2 + geom_point()

# plot the linear regression model
p2 <- p2 + geom_smooth(method='lm', color='turquoise4')

# change theme
p2 <- p2 + theme_minimal()

# add title
p2 <- p2 + labs(title="Linear model of points vs attitude")

# adjust title
p2 <- p2 + theme(plot.title = element_text(hjust=0.5, size=16, face='bold')) 

# draw the plot
p2

```

## Interpretation of the fitted model

By removing the insignificant variables from the model, I obtain a fit with an even lower P-value. Seems like `attitude` affects `points` with a high statistical significance. On average, for each unit `attitude`, `points` increases by 3.5255 with a standard error of 0.5674. For vanishing `attitude` (an extrapolation to 0), the expected `points` is 11.6372 with a standard error of 1.8303. The multiple R-squared, which in this case with a single explanatory variable is simply the square of the correlation coefficient between `attitude` and `points`, is 0.1906, indicating that about 19% of the variation in `points` is explained by the variation in `attitude`. From this I can conclude that `attitude` has a significant impact on `points` but still the majority of the variance in `points` is explained by other factors not included in the data set.

## Diagnostic plots

Finally, I produce some diagnostic plots:

```{r}
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2), mai=c(0.8,0.8,0.2,0.2), cex=0.8, pty="m")
plot(my_model2, which = c(1,2,5))

```

From the plots I can interpret that

-    *Residuals vs Fitted*: Residuals seems to be centered around the model with an approximately constant variance so the assumptions of linearity and constant variance hold.
-    *Q-Q Residuals*: The circles on the quantile-quantile plot line up fairly well on the theoretical line meaning that the assumption of normally distributed model residuals holds. The plot essentially compares the distribution of residuals from the data (circles) with the normal distribution of the model residuals (dashed line).
-    *Residuals vs Leverage*: All the data points fall inside the Cook's distance so there are not any influential points in the regression model. If there was an influential point, removing it from the data set would considerably change the coefficients of the model.

















