# Clustering and classification of Boston data set (Week 4)

In this Analysis exercise, I study the Boston set from the MASS package mostly focusing on crime rates in suburbs of Boston.

## Loading the data

First, I load the Boston data set from the MASS package and explore its structure and dimensions,

```{r}

# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)

```

There are 506 observations and 14 numeric variables that are related to housing in suburbs of Boston:

- `crim`: per capita crime rate by town,
- `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.,
- `indus`: proportion of non-retail business acres per town,
- `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise),
- `nox`: nitrogen oxides concentration (parts per 10 million),
- `rm`: average number of rooms per dwelling,
- `age`: proportion of owner-occupied units built prior to 1940,
- `dis`: weighted mean of distances to five Boston employment centres,
- `rad`: index of accessibility to radial highways,
- `tax`: full-value property-tax rate per $10,000,
- `ptratio`: pupil-teacher ratio by town,
- `black`: $1000(Bkâˆ’0.63)^2$ $Bk$ is the proportion of blacks by town,
- `lstat`: lower status of the population (percent),
- `medv`: median value of owner-occupied homes in $1000s.

## Graphical overview of the data

Next, I give a visual representation of the distributions of the different variables and correlations between them. Let's start by summarizing the variables,

```{r}

# print out the summary of the variables
summary(Boston)

```

A more visual way to study the distributions of the variables is to draw density plots,

```{r}

# access the tidyverse libraries tidyr, dplyr, ggplot2
library(tidyr); library(dplyr); library(ggplot2);

# plot the distributions as densities
Boston %>% gather %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_density()

```

In the distribution plots, I can see that at least the distribution of `rm` is somewhat normally distributed, while the others are far from a normal distribution. Most of the distributions are positively skewed: `chas`, `crim`, `dis`, `lstat`, `medv`, `nox`, and `zn`. On the other hand, `age` and `black` are negatively skewed. Further, three distributions have double peaks: `indus`, `rad`, and `tax`.

Now that we know how the variables are distributed, let's consider the correlations between them and create a correlation matrix by using the function `cor` and rounding the resulting numbers to 2-decimal precision:

```{r}

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(2)

# print the correlation matrix
cor_matrix

```

To illustrate the correlations between the variables in a more visual manner, I plot the above matrix using the `corrplot` function from the `corrplot` library,

```{r}

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos = "d", tl.cex=0.6)

```

As seen in the correlation plot, `rad` and `tax` have the strongest correlation, which indicates that properties that have a good access to radial highways have higher tax rates. Second highest correlation is between `indus` and `nox`, which makes sense since industry might produce nitrogen oxides. The strongest anti-correlation, however, is between `dis` and `nox` indicating that there are less nitrogen oxides at residences further away from the employment centers. The second highest anti-correlation is between `age` and `dis` meaning that residences near employment centers are typically older.

## Creating train and test sets

Next, I create train and test sets for studying the `crim` variable. First, I standardize the data set using the `scale` function which subtracts the column means from the corresponding columns and divides the difference with standard deviation, i.e.

$$
\text{scaled}(X) = \frac{X-\text{mean}(X)}{\text{sd}(X)}.
$$

Scaling the variables and printing out the summaries yields

```{r}

# center and standardize variables
boston_scaled <- Boston %>% scale

# summaries of the scaled variables
boston_scaled %>% summary

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

To proceed, I create a categorical variable `crime` from the variable `crim` using the quantiles as the break points. Then I remove `crim` from the data set and add there the new categorical variable `crime`,

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

Finally, I divide the data set into train and test sets sot that 80% of the data belongs to the train set, 

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# fix the seed for reproducibility
set.seed(12321)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

## Fitting the LDA model

Next, I fit a linear discriminant analysis (LDA) model to the train set using `crime` as the target variable and all the other variables as predictors. The model finds the (linear) combination of the variables that separate the four classes of `crime`. The result of the fit is

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

```

The resulting LDA model can be plotted as a biplot by first creating a function for `lda.arrows` and then plotting `lda.fit` with the created arrows,

```{r}

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

According to the biplot, `rad` plays a significant role when distinguishing the category `high` from the rest. One possible reason for this is that crimes are concentrated to some specific areas that are far from radial highways. Among the other three categories, `zn`, `nox`, and `medv` have a major impact to the division.

## Predicting with the LDA model

Now that I have the LDA model, I can use it to predict crime rates from the other variables. To do that, I first save and remove the correct crime categories from the test set,

```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

Now, I can use `lda.pred` to predict the categorical crime rates of the test set and cross-tabulate the predicted results with the correct crime rates that I just saved,

```{r}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

It seems like the LDA predicts the `high` category flawlessly but with the other three categories there are some mispredictions indicated by the nonzero off-diagonal components. However, the largest values are on the diagonal, which indicates that the model is performing somewhat well.

## K-means clustering

As a final task, I apply the K-means clustering algorithm to the Boston data set in order to group similar observations into clusters. The (Euclidean) distances between the observations are

```{r}

# reload Boston data set
data("Boston")

# center and standardize variables
boston_scaled2 <- Boston %>% scale

# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

```

The K-means clustering can be obtained with the function `kmeans` which actually computes the distances automatically. To determine an optimal value for the number of clusters, I look at how the total of within cluster sum of squares (WCSS) behaves when the number of clusters changes. For the optimal number of clusters, the total WCSS drops radically as a function of the number of clusters. Computing the total WCSS and plotting the result as a function of the number of clusters yields

```{r}

# set seed for reproducibility
set.seed(1334)

# determine the maximum number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

From the plot I can see that the optimal number of clusters could be two in this case. Also seven could be an optimal value but I choose two since the drop is the steepest when coming from one to two. Using two clusters, the k-means clustering yields

```{r}

# k-means clustering
km <- kmeans(boston_scaled2, centers = 2)

```

Now I can plot the result of the clustering using `pairs` and use different colors for the two clusters. I focus on five variables that are related to the crime rates according to the previous analysis: `crim`, `rad`, `nox`, `zn`, and `medv`,

```{r}

# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)

# plot the Boston dataset with clusters
pairs(boston_scaled2[c("crim", "rad", "nox", "zn", "medv")], col = km$cluster)

```

Looking at the plot, I observe that the clustering is visible in many pairs of variables. This is especially true with pairs including `crim` probably because the four other variables play a major role when classifying the crime rates as we saw in the previous analysis. 









