# Clustering and classification of Boston data set (Week 4)

In this Analysis exercise, ...

## Loading the data

First, I load the Boston data set from the MASS package and explore its structure and dimensions,

```{r}

# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)

```

There are 506 observations and 14 numeric variables that are related to housing in suburbs of Boston:

- `crim`: per capita crime rate by town,
- `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.,
- `indus`: proportion of non-retail business acres per town,
- `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise),
- `nox`: nitrogen oxides concentration (parts per 10 million),
- `rm`: average number of rooms per dwelling,
- `age`: proportion of owner-occupied units built prior to 1940,
- `dis`: weighted mean of distances to five Boston employment centres,
- `rad`: index of accessibility to radial highways,
- `tax`: full-value property-tax rate per $10,000,
- `ptratio`: pupil-teacher ratio by town,
- `black`: $1000(Bkâˆ’0.63)^2$ $Bk$ is the proportion of blacks by town,
- `lstat`: lower status of the population (percent),
- `medv`: median value of owner-occupied homes in $1000s.

## Graphical overview of the data

Next, I give a visual representation of the distributions of the different variables and correlations between them. Let's start by summarizing the variables,

```{r}

# print out the summary of the variables
summary(Boston)

```

A more visual way to study the distributions of the variables is to draw density plots,

```{r}

# access the tidyverse libraries tidyr, dplyr, ggplot2
library(tidyr); library(dplyr); library(ggplot2);

# plot the distributions as densities
Boston %>% gather %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_density()

```

In the distribution plots, I can see that at least the distribution of `rm` is somewhat normally distributed, while the others are far from a normal distribution. Most of the distributions are positively skewed: `chas`, `crim`, `dis`, `lstat`, `medv`, `nox`, and `zn`. On the other hand, `age` and `black` are negatively skewed. Further, three distributions have double peaks: `indus`, `rad`, and `tax`.



Now that we know how the variables are distributed, let's consider the correlations between them and create a correlation matrix by using the function `cor` and rounding the resulting numbers to 2-decimal precision:

```{r}

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(2)

# print the correlation matrix
cor_matrix

```

To illustrate the correlations between the variables in a more visual manner, I plot the above matrix using the `corrplot` function from the `corrplot` library,

```{r}

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos = "d", tl.cex=0.6)

```

As seen in the correlation plot, `rad` and `tax` have the strongest correlation, which indicates that properties that have a good access to radial highways have higher tax rates. Second highest correlation is between `indus` and `nox`, which makes sense since industry might produce nitrogen oxides. The strongest anti-correlation, however, is between `dis` and `nox` indicating that there are less nitrogen oxides at residences further away from the employment centers. The second highest anti-correlation is between `age` and `dis` meaning that residences near employment centers are typically older.

## Creating train and test sets

Next, I create train and test sets for studying the `crim` variable. First, I standardize the data set using the `scale` function which subtracts the column means from the corresponding columns and divides the difference with standard deviation, i.e.

$$
\text{scaled}(X) = \frac{X-\text{mean}(X)}{\text{sd}(X)}.
$$

Scaling the variables and printing out the summaries yields

```{r}

# center and standardize variables
boston_scaled <- Boston %>% scale

# summaries of the scaled variables
boston_scaled %>% summary

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

To proceed, I create a categorical variable `crime` from the variable `crim` using the quantiles as the break points. Then I remove `crim` from the data set and add there the new categorical variable `crime`,

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```



```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```











